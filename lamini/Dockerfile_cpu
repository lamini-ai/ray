FROM rayproject/ray:2.44.0-py312-cpu

RUN pip install --no-cache-dir \
    'ray[serve,llm]>=2.44.0' \
    'vllm>=0.7.2' \
    xgrammar==0.1.11 \
    pynvml==12.0.0 \
    awscli asyncache httpx

# Define Python path as a variable for easier maintenance
ENV PYTHON_PATH=/home/ray/anaconda3/lib/python3.12/site-packages

# replace `lora_model_loader.py`, support load loRA model from local file
COPY python/ray/llm/_internal/serve/deployments/llm/multiplex/lora_model_loader.py ${PYTHON_PATH}/ray/llm/_internal/serve/deployments/llm/multiplex/lora_model_loader.py

# replace `json_mode_utils.py`, support json_mode unevaluatedProperties True
COPY python/ray/llm/_internal/serve/configs/json_mode_utils.py ${PYTHON_PATH}/ray/llm/_internal/serve/configs/json_mode_utils.py

# replace `vllm_engine.py`, add more logging for vllm engine
COPY python/ray/llm/_internal/serve/deployments/llm/vllm/vllm_engine.py ${PYTHON_PATH}/ray/llm/_internal/serve/deployments/llm/vllm/vllm_engine.py

# # replace `vllm_models.py`, add MoME
# COPY python/ray/llm/_internal/serve/deployments/llm/vllm/vllm_models.py ${PYTHON_PATH}/ray/llm/_internal/serve/deployments/llm/vllm/vllm_models.py

# # replace `llm_server.py`, add MoME
# COPY python/ray/llm/_internal/serve/deployments/llm/llm_server.py ${PYTHON_PATH}/ray/llm/_internal/serve/deployments/llm/llm_server.py

# # replace `vllm_engine.py`, add MoME
# COPY python/ray/llm/_internal/serve/deployments/llm/vllm/vllm_engine.py ${PYTHON_PATH}/ray/llm/_internal/serve/deployments/llm/vllm/vllm_engine.py
